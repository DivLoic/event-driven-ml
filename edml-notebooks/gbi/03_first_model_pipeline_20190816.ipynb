{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\"dayofweek\", \"hourofday\", \"pickup_borough\", \"dropoff_borough\", \"trip_duration\"]\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "DEFAULTS = [[1], [0], [\"\"], [\"\"],  []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(records):\n",
    "            columns = tf.decode_csv(records, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            features[\"dayofweek\"] = features[\"dayofweek\"] - 1\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(file_list,compression_type=\"GZIP\")  # Read text file\n",
    "                   .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = 5 # None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size, seed=42)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "def get_train_input_fn():\n",
    "    return read_dataset('gs://edml/data/taxi-trips/train/tlc_yellow_trips_2018-000*.csv',\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "    return read_dataset('gs://edml/data/taxi-trips/val/tlc_yellow_trips_2018-000*.csv',\n",
    "                        mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "    \n",
    "    borough_list = [\"Manhattan\", \"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\", \"EWR\"]\n",
    "        \n",
    "    # One hot encode categorical features\n",
    "    fc_dayofweek = tf.feature_column.categorical_column_with_identity(key=\"dayofweek\", num_buckets = 7)\n",
    "    fc_hourofday = tf.feature_column.categorical_column_with_identity(key=\"hourofday\", num_buckets = 24)\n",
    "    fc_pickuploc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"pickup_borough\", \n",
    "                                                                             vocabulary_list=borough_list)\n",
    "    fc_dropoffloc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dropoff_borough\", \n",
    "                                                                              vocabulary_list=borough_list)\n",
    "    \n",
    "    # Cross features to get combination of day and hour and pickup-dropoff locations\n",
    "    fc_crossed_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "    fc_crossed_pd_pair = tf.feature_column.crossed_column(keys = [fc_pickuploc, fc_dropoffloc], hash_bucket_size = 6*6)\n",
    "    \n",
    "    wide = [\n",
    "        # Feature crosses\n",
    "        fc_crossed_day_hr, fc_crossed_pd_pair,\n",
    "        \n",
    "        # Sparse columns\n",
    "        fc_dayofweek, fc_hourofday,\n",
    "        fc_pickuploc, fc_dropoffloc\n",
    "    ]\n",
    "    \n",
    "    # Embedding_column to \"group\" together ...\n",
    "    fc_embed_pd_pair = tf.feature_column.embedding_column(categorical_column = fc_crossed_pd_pair, dimension = 4)\n",
    "    fc_embed_day_hr = tf.feature_column.embedding_column(categorical_column = fc_crossed_day_hr, dimension = 16)\n",
    "    \n",
    "    deep = [\n",
    "        fc_embed_pd_pair,\n",
    "        fc_embed_day_hr\n",
    "    ]\n",
    "    \n",
    "    return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving input receiver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(dtype = tf.int64, shape = [None], name=\"dayofweek\"),\n",
    "        'hourofday' : tf.placeholder(dtype = tf.int64, shape = [None], name=\"hourofday\"),\n",
    "        'pickup_borough' : tf.placeholder(dtype = tf.string, shape = [None], name=\"pickup_borough\"), \n",
    "        'dropoff_borough' : tf.placeholder(dtype = tf.string, shape = [None], name=\"dropoff_borough\"),\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in receiver_tensors.items()\n",
    "    }\n",
    "        \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(output_dir):\n",
    "    \n",
    "    EVAL_INTERVAL = 300\n",
    "    wide, deep = get_wide_deep()\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                        tf_random_seed = 2810,\n",
    "                                        keep_checkpoint_max = 3)\n",
    "    \n",
    "    # Add custom evaluation metric\n",
    "    def my_rmse(labels, predictions):\n",
    "        pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "        return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = output_dir,\n",
    "        linear_feature_columns = wide,\n",
    "        dnn_feature_columns = deep,\n",
    "        dnn_hidden_units = [128, 64, 32],\n",
    "        config = run_config)\n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator = estimator, metric_fn = my_rmse) \n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = get_train_input_fn(),\n",
    "        max_steps = 500)\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_receiver_fn = serving_input_receiver_fn)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = get_valid_input_fn(),\n",
    "        steps = None,\n",
    "        start_delay_secs = 60, # start evaluating after N seconds\n",
    "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"gs://edml/data/taxi-trips/model_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# gsutil rm gs://bucket/subdir/** will remove all objects under gs://bucket/subdir or any of its subdirectories.\n",
    "gsutil rm gs://edml/data/taxi-trips/model_test/** # start fresh each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
