{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\"dayofweek\", \"hourofday\", \"pickup_borough\", \"dropoff_borough\", \"trip_duration\"]\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "DEFAULTS = [[1], [0], [\"\"], [\"\"],  []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(records):\n",
    "            columns = tf.decode_csv(records, record_defaults=DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            features[\"dayofweek\"] = features[\"dayofweek\"] - 1\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "        \n",
    "        # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = (tf.data.TextLineDataset(file_list,compression_type=\"GZIP\")  # Read text file\n",
    "                   .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = 5 # None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size, seed=42)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "def get_train_input_fn():\n",
    "    return read_dataset('gs://edml/data/taxi-trips/train/tlc_yellow_trips_2018-000*.csv',\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "    return read_dataset('gs://edml/data/taxi-trips/val/tlc_yellow_trips_2018-000*.csv',\n",
    "                        mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "    \n",
    "    borough_list = [\"Manhattan\", \"Queens\", \"Brooklyn\", \"Bronx\", \"Staten Island\", \"EWR\"]\n",
    "        \n",
    "    # One hot encode categorical features\n",
    "    fc_dayofweek = tf.feature_column.categorical_column_with_identity(key=\"dayofweek\", num_buckets = 7)\n",
    "    fc_hourofday = tf.feature_column.categorical_column_with_identity(key=\"hourofday\", num_buckets = 24)\n",
    "    fc_pickuploc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"pickup_borough\", \n",
    "                                                                             vocabulary_list=borough_list)\n",
    "    fc_dropoffloc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dropoff_borough\", \n",
    "                                                                              vocabulary_list=borough_list)\n",
    "    \n",
    "    # Cross features to get combination of day and hour and pickup-dropoff locations\n",
    "    fc_crossed_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "    fc_crossed_pd_pair = tf.feature_column.crossed_column(keys = [fc_pickuploc, fc_dropoffloc], hash_bucket_size = 6*6)\n",
    "    \n",
    "    wide = [\n",
    "        # Feature crosses\n",
    "        fc_crossed_day_hr, fc_crossed_pd_pair,\n",
    "        \n",
    "        # Sparse columns\n",
    "        fc_dayofweek, fc_hourofday,\n",
    "        fc_pickuploc, fc_dropoffloc\n",
    "    ]\n",
    "    \n",
    "    # Embedding_column to \"group\" together ...\n",
    "    fc_embed_pd_pair = tf.feature_column.embedding_column(categorical_column = fc_crossed_pd_pair, dimension = 4)\n",
    "    fc_embed_day_hr = tf.feature_column.embedding_column(categorical_column = fc_crossed_day_hr, dimension = 16)\n",
    "    \n",
    "    deep = [\n",
    "        fc_embed_pd_pair,\n",
    "        fc_embed_day_hr\n",
    "    ]\n",
    "    \n",
    "    return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving input receiver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function to be able to serve predictions later using provided inputs\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(dtype = tf.int64, shape = [None], name=\"dayofweek\"),\n",
    "        'hourofday' : tf.placeholder(dtype = tf.int64, shape = [None], name=\"hourofday\"),\n",
    "        'pickup_borough' : tf.placeholder(dtype = tf.string, shape = [None], name=\"pickup_borough\"), \n",
    "        'dropoff_borough' : tf.placeholder(dtype = tf.string, shape = [None], name=\"dropoff_borough\"),\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in receiver_tensors.items()\n",
    "    }\n",
    "        \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator to train and evaluate\n",
    "def train_and_evaluate(output_dir):\n",
    "    \n",
    "    EVAL_INTERVAL = 300\n",
    "    wide, deep = get_wide_deep()\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
    "                                        tf_random_seed = 2810,\n",
    "                                        keep_checkpoint_max = 3)\n",
    "    \n",
    "    # Add custom evaluation metric\n",
    "    def my_rmse(labels, predictions):\n",
    "        pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "        return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir = output_dir,\n",
    "        linear_feature_columns = wide,\n",
    "        dnn_feature_columns = deep,\n",
    "        dnn_hidden_units = [128, 64, 32],\n",
    "        config = run_config)\n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator = estimator, metric_fn = my_rmse) \n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = get_train_input_fn(),\n",
    "        max_steps = 500)\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_receiver_fn = serving_input_receiver_fn)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = get_valid_input_fn(),\n",
    "        steps = None,\n",
    "        start_delay_secs = 60, # start evaluating after N seconds\n",
    "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters = exporter)\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"gs://edml/data/taxi-trips/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://edml/data/taxi-trips/model/...\n",
      "Removing gs://edml/data/taxi-trips/model/checkpoint...                          \n",
      "Removing gs://edml/data/taxi-trips/model/eval/...                               \n",
      "Removing gs://edml/data/taxi-trips/model/eval/events.out.tfevents.1572293364.tensorflow-20191018...\n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://edml/data/taxi-trips/model/events.out.tfevents.1572292709.tensorflow-20191018...\n",
      "Removing gs://edml/data/taxi-trips/model/export/...                             \n",
      "Removing gs://edml/data/taxi-trips/model/export/exporter/...                    \n",
      "Removing gs://edml/data/taxi-trips/model/export/exporter/temp-b'1572293366'/... \n",
      "Removing gs://edml/data/taxi-trips/model/graph.pbtxt...                         \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-0.data-00000-of-00002...    \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-0.data-00001-of-00002...    \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-0.index...                  \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-0.meta...                   \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-500.data-00000-of-00002...  \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-500.data-00001-of-00002...  \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-500.index...                \n",
      "Removing gs://edml/data/taxi-trips/model/model.ckpt-500.meta...                 \n",
      "/ [17 objects]                                                                  \n",
      "Operation completed over 17 objects.                                             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# gsutil rm gs://bucket/subdir/** will remove all objects under gs://bucket/subdir or any of its subdirectories.\n",
    "gsutil rm gs://edml/data/taxi-trips/model** # start fresh each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 300, '_experimental_distribute': None, '_num_ps_replicas': 0, '_eval_distribute': None, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_global_id_in_cluster': 0, '_experimental_max_worker_delay_secs': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_service': None, '_is_chief': True, '_task_type': 'worker', '_save_summary_steps': 100, '_tf_random_seed': 2810, '_train_distribute': None, '_master': '', '_log_step_count_steps': 100, '_model_dir': 'gs://edml/data/taxi-trips/model', '_evaluation_master': '', '_protocol': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd84aebe9e8>}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 300, '_experimental_distribute': None, '_num_ps_replicas': 0, '_eval_distribute': None, '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_global_id_in_cluster': 0, '_experimental_max_worker_delay_secs': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_service': None, '_is_chief': True, '_task_type': 'worker', '_save_summary_steps': 100, '_tf_random_seed': 2810, '_train_distribute': None, '_master': '', '_log_step_count_steps': 100, '_model_dir': 'gs://edml/data/taxi-trips/model', '_evaluation_master': '', '_protocol': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd84aebe940>}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into gs://edml/data/taxi-trips/model/model.ckpt.\n",
      "INFO:tensorflow:loss = 8116340.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 21.4575\n",
      "INFO:tensorflow:loss = 86455.76, step = 101 (4.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.4773\n",
      "INFO:tensorflow:loss = 1237928.2, step = 201 (3.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.5236\n",
      "INFO:tensorflow:loss = 2306942.5, step = 301 (3.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 28.9101\n",
      "INFO:tensorflow:loss = 5935324.0, step = 401 (3.459 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into gs://edml/data/taxi-trips/model/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-10-28T20:11:50Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://edml/data/taxi-trips/model/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-10-28-20:22:21\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 4487.6475, global_step = 500, label/mean = 17.492971, loss = 2297623.8, prediction/mean = 12.437951, rmse = 66.98991\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: gs://edml/data/taxi-trips/model/model.ckpt-500\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'pickup_borough': <tf.Tensor 'pickup_borough:0' shape=(?,) dtype=string>, 'dayofweek': <tf.Tensor 'dayofweek:0' shape=(?,) dtype=int64>, 'hourofday': <tf.Tensor 'hourofday:0' shape=(?,) dtype=int64>, 'dropoff_borough': <tf.Tensor 'dropoff_borough:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'pickup_borough': <tf.Tensor 'pickup_borough:0' shape=(?,) dtype=string>, 'dayofweek': <tf.Tensor 'dayofweek:0' shape=(?,) dtype=int64>, 'hourofday': <tf.Tensor 'hourofday:0' shape=(?,) dtype=int64>, 'dropoff_borough': <tf.Tensor 'dropoff_borough:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from gs://edml/data/taxi-trips/model/model.ckpt-500\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://edml/data/taxi-trips/model/export/exporter/temp-b'1572294143'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 92072.46.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
