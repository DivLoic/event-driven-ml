{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"event-driven-ml\"\n",
    "DATASET_GCP_PROJECT_ID = \"event-driven-ml\"\n",
    "DATASET_ID = \"edml_nyc_yellow_taxi_us\"\n",
    "TABLE_ID = \"viz_gis_feat_eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    \"uuid\", \"dayofweek\", \"hourofday\", \"weekofyear\", \"pickup_zone_name\", \"dropoff_zone_name\", \"centroide_distance\",\n",
    "    \"flag_airport\", \"flag_5_seat_car\", \"trip_duration\"\n",
    "]\n",
    "LABEL_COLUMN = \"trip_duration\"\n",
    "KEY_COLUMN = \"uuid\"\n",
    "\n",
    "# Set default values for each column\n",
    "DEFAULTS = [\n",
    "    ['no_key'], [7], [25], [26], [\"\"], [\"\"], [0.0], [2], [2], []\n",
    "]\n",
    "\n",
    "NEMBEDS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "def read_dataset(suffix, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        client = BigQueryClient()\n",
    "        read_session = client.read_session(\n",
    "            parent=\"projects/\" + PROJECT_ID,\n",
    "            project_id=DATASET_GCP_PROJECT_ID,\n",
    "            table_id=\"{}_{}\".format(TABLE_ID, suffix), \n",
    "            dataset_id=DATASET_ID,\n",
    "            selected_fields=INPUT_COLUMNS,\n",
    "            output_types=[\n",
    "                tf.string, tf.int64, tf.int64, tf.int64, tf.string, tf.string, tf.float64, tf.int64, tf.int64, tf.int64\n",
    "            ],\n",
    "            requested_streams=10\n",
    "        )\n",
    "        \n",
    "        def decode_row(records):\n",
    "            features = records\n",
    "            label = tf.cast(features.pop(LABEL_COLUMN), tf.float32)\n",
    "            return features, label\n",
    "        \n",
    "        dataset = read_session.parallel_read_rows(sloppy=True).map(decode_row)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=1000*batch_size, seed=1234)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY = [\n",
    "    'Allerton/Pelham Gardens', 'Alphabet City', 'Arden Heights', 'Arrochar/Fort Wadsworth', \n",
    "    'Astoria', 'Astoria Park', 'Auburndale', 'Baisley Park', 'Bath Beach', 'Battery Park',\n",
    "    'Battery Park City', 'Bay Ridge', 'Bay Terrace/Fort Totten', 'Bayside', 'Bedford', \n",
    "    'Bedford Park', 'Bellerose', 'Belmont', 'Bensonhurst East', 'Bensonhurst West',\n",
    "    'Bloomfield/Emerson Hill', 'Bloomingdale', 'Boerum Hill', 'Borough Park', \n",
    "    'Breezy Point/Fort Tilden/Riis Beach', 'Briarwood/Jamaica Hills', 'Brighton Beach',\n",
    "    'Broad Channel', 'Bronx Park', 'Bronxdale', 'Brooklyn Heights', 'Brooklyn Navy Yard', \n",
    "    'Brownsville', 'Bushwick North', 'Bushwick South', 'Cambria Heights', 'Canarsie', 'Carroll Gardens',\n",
    "    'Central Harlem', 'Central Harlem North', 'Central Park', 'Charleston/Tottenville', 'Chinatown',\n",
    "    'City Island', 'Claremont/Bathgate', 'Clinton East', 'Clinton Hill', 'Clinton West', 'Co-Op City',\n",
    "    'Cobble Hill', 'College Point', 'Columbia Street', 'Coney Island', 'Corona', 'Country Club', 'Crotona Park',\n",
    "    'Crotona Park East', 'Crown Heights North', 'Crown Heights South', 'Cypress Hills', 'DUMBO/Vinegar Hill',\n",
    "    'Douglaston', 'Downtown Brooklyn/MetroTech', 'Dyker Heights', 'East Chelsea', 'East Concourse/Concourse Village',\n",
    "    'East Elmhurst', 'East Flatbush/Farragut', 'East Flatbush/Remsen Village', 'East Flushing', 'East Harlem North',\n",
    "    'East Harlem South', 'East New York', 'East New York/Pennsylvania Avenue', 'East Tremont', 'East Village',\n",
    "    'East Williamsburg', 'Eastchester', 'Elmhurst', 'Elmhurst/Maspeth', \"Eltingville/Annadale/Prince's Bay\", 'Erasmus',\n",
    "    'Far Rockaway', 'Financial District North', 'Financial District South', 'Flatbush/Ditmas Park', 'Flatiron',\n",
    "    'Flatlands', 'Flushing', 'Flushing Meadows-Corona Park', 'Fordham South', 'Forest Hills',\n",
    "    'Forest Park/Highland Park', 'Fort Greene', 'Fresh Meadows', 'Freshkills Park', 'Garment District', 'Glen Oaks',\n",
    "    'Glendale', 'Gowanus', 'Gramercy', 'Gravesend', 'Great Kills', 'Great Kills Park', 'Green-Wood Cemetery',\n",
    "    'Greenpoint', 'Greenwich Village North', 'Greenwich Village South', 'Grymes Hill/Clifton', 'Hamilton Heights',\n",
    "    'Hammels/Arverne', 'Heartland Village/Todt Hill', 'Highbridge', 'Highbridge Park', 'Hillcrest/Pomonok', 'Hollis',\n",
    "    'Homecrest', 'Howard Beach', 'Hudson Sq', 'Hunts Point', 'Inwood', 'Inwood Hill Park', 'JFK Airport',\n",
    "    'Jackson Heights', 'Jamaica', 'Jamaica Bay', 'Jamaica Estates', 'Kensington', 'Kew Gardens', 'Kew Gardens Hills',\n",
    "    'Kingsbridge Heights', 'Kips Bay', 'LaGuardia Airport', 'Laurelton', 'Lenox Hill East', 'Lenox Hill West',\n",
    "    'Lincoln Square East', 'Lincoln Square West', 'Little Italy/NoLiTa', 'Long Island City/Hunters Point',\n",
    "    'Long Island City/Queens Plaza', 'Longwood', 'Lower East Side', 'Madison', 'Manhattan Beach', 'Manhattan Valley',\n",
    "    'Manhattanville', 'Marble Hill', 'Marine Park/Floyd Bennett Field', 'Marine Park/Mill Basin', 'Mariners Harbor',\n",
    "    'Maspeth', 'Meatpacking/West Village West', 'Melrose South', 'Middle Village', 'Midtown Center', 'Midtown East',\n",
    "    'Midtown North', 'Midtown South', 'Midwood', 'Morningside Heights', 'Morrisania/Melrose', 'Mott Haven/Port Morris',\n",
    "    'Mount Hope', 'Murray Hill', 'Murray Hill-Queens', 'New Dorp/Midland Beach', 'Newark Airport', 'North Corona',\n",
    "    'Norwood', 'Oakland Gardens', 'Oakwood', 'Ocean Hill', 'Ocean Parkway South', 'Old Astoria', 'Ozone Park',\n",
    "    'Park Slope', 'Parkchester', 'Pelham Bay', 'Pelham Bay Park', 'Pelham Parkway', 'Penn Station/Madison Sq West',\n",
    "    'Port Richmond', 'Prospect Heights', 'Prospect Park', 'Prospect-Lefferts Gardens', 'Queens Village',\n",
    "    'Queensboro Hill', 'Queensbridge/Ravenswood', 'Randalls Island', 'Red Hook', 'Rego Park', 'Richmond Hill',\n",
    "    'Ridgewood', 'Rikers Island', 'Riverdale/North Riverdale/Fieldston', 'Rockaway Park', 'Roosevelt Island', 'Rosedale',\n",
    "    'Rossville/Woodrow', 'Saint Albans', 'Saint George/New Brighton', 'Saint Michaels Cemetery/Woodside',\n",
    "    'Schuylerville/Edgewater Park', 'Seaport', 'Sheepshead Bay', 'SoHo', 'Soundview/Bruckner', 'Soundview/Castle Hill',\n",
    "    'South Beach/Dongan Hills', 'South Jamaica', 'South Ozone Park', 'South Williamsburg', 'Springfield Gardens North',\n",
    "    'Springfield Gardens South', 'Spuyten Duyvil/Kingsbridge', 'Stapleton', 'Starrett City', 'Steinway',\n",
    "    'Stuy Town/Peter Cooper Village', 'Stuyvesant Heights', 'Sunnyside', 'Sunset Park East', 'Sunset Park West',\n",
    "    'Sutton Place/Turtle Bay North', 'Times Sq/Theatre District', 'TriBeCa/Civic Center', 'Two Bridges/Seward Park',\n",
    "    'UN/Turtle Bay South', 'Union Sq', 'University Heights/Morris Heights', 'Upper East Side North',\n",
    "    'Upper East Side South', 'Upper West Side North', 'Upper West Side South', 'Van Cortlandt Park',\n",
    "    'Van Cortlandt Village', 'Van Nest/Morris Park', 'Washington Heights North', 'Washington Heights South',\n",
    "    'West Brighton', 'West Chelsea/Hudson Yards', 'West Concourse', 'West Farms/Bronx River', 'West Village',\n",
    "    'Westchester Village/Unionport', 'Westerleigh', 'Whitestone', 'Willets Point', 'Williamsbridge/Olinville',\n",
    "    'Williamsburg (North Side)', 'Williamsburg (South Side)', 'Windsor Terrace', 'Woodhaven', 'Woodlawn/Wakefield',\n",
    "    'Woodside', 'World Trade Center', 'Yorkville East', 'Yorkville West'\n",
    "]\n",
    "\n",
    "_CATEGORICAL_STR_VOCAB = {\n",
    "    \"pickup_zone_name\": VOCABULARY,\n",
    "    \"dropoff_zone_name\": VOCABULARY\n",
    "}\n",
    "\n",
    "_CATEGORICAL_NUM_BUCKETS = {\n",
    "    \"dayofweek\": 7,\n",
    "    \"hourofday\": 24,\n",
    "    \"weekofyear\": 26 #53\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "    \n",
    "    # numerical column\n",
    "    fc_distance = tf.feature_column.numeric_column(\"centroide_distance\")\n",
    "    \n",
    "    # One hot encode categorical features\n",
    "    fc_dayofweek = tf.feature_column.categorical_column_with_identity(key=\"dayofweek\",\n",
    "                                                                                num_buckets=_CATEGORICAL_NUM_BUCKETS[\"dayofweek\"])\n",
    "    \n",
    "    fc_hourofday = tf.feature_column.categorical_column_with_identity(key=\"hourofday\",\n",
    "                                                                                num_buckets=_CATEGORICAL_NUM_BUCKETS[\"hourofday\"])\n",
    "    \n",
    "    fc_weekofyear = tf.feature_column.categorical_column_with_identity(key=\"weekofyear\",\n",
    "                                                                                 num_buckets=_CATEGORICAL_NUM_BUCKETS[\"weekofyear\"])\n",
    "    # 2 steps to one hot encoding\n",
    "    fc_pickuploc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"pickup_zone_name\",\n",
    "                                                                                vocabulary_list=_CATEGORICAL_STR_VOCAB[\"pickup_zone_name\"])\n",
    "    fc_pickuploc_ooe = tf.feature_column.indicator_column(fc_pickuploc)\n",
    "    \n",
    "    fc_dropoffloc = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dropoff_zone_name\",\n",
    "                                                                            vocabulary_list=_CATEGORICAL_STR_VOCAB[\"pickup_zone_name\"])\n",
    "    \n",
    "    fc_dropoffloc_ooe = tf.feature_column.indicator_column(fc_dropoffloc)\n",
    "    \n",
    "    # Cross features to get combination of day and hour and pickup-dropoff locations\n",
    "    fc_crossed_day_hr = tf.feature_column.crossed_column(keys=[fc_dayofweek, fc_hourofday], hash_bucket_size=100)\n",
    "    fc_crossed_pd_pair = tf.feature_column.crossed_column(keys=[fc_pickuploc, fc_dropoffloc], hash_bucket_size=10000)\n",
    "    fc_crossed_hour_pu = tf.feature_column.crossed_column(keys=[fc_hourofday, fc_pickuploc], hash_bucket_size=1000)\n",
    "    fc_crossed_hour_df = tf.feature_column.crossed_column(keys=[fc_hourofday, fc_dropoffloc], hash_bucket_size=1000)\n",
    "    \n",
    "    # binarize feature cross\n",
    "    fc_crossed_day_hr_ooe = tf.feature_column.indicator_column(fc_crossed_day_hr)\n",
    "    fc_crossed_pd_pair_ooe = tf.feature_column.indicator_column(fc_crossed_pd_pair)\n",
    "    fc_crossed_hour_pu_ooe = tf.feature_column.indicator_column(fc_crossed_hour_pu)\n",
    "    fc_crossed_hour_df_ooe = tf.feature_column.indicator_column(fc_crossed_hour_df)\n",
    "    \n",
    "    fc_airport = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key=\"flag_airport\", num_buckets=2))\n",
    "    fc_big_car = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key=\"flag_5_seat_car\", num_buckets=2))\n",
    "    fc_distance_cat = tf.feature_column.bucketized_column(source_column=fc_distance, \n",
    "                                                          boundaries=[1248.0, 2077.0, 3778.0])\n",
    "    fc_distance_ooe = tf.feature_column.indicator_column(fc_distance_cat)\n",
    "    \n",
    "    wide = [        \n",
    "        # Sparse columns\n",
    "        fc_weekofyear, fc_crossed_day_hr_ooe, fc_crossed_pd_pair_ooe, fc_crossed_hour_pu_ooe, fc_crossed_hour_df_ooe, fc_airport, fc_big_car, \n",
    "        fc_distance_ooe\n",
    "    ]\n",
    "        \n",
    "    # Embedding_column\n",
    "    fc_embed_weekofyear = tf.compat.v1.feature_column.embedding_column(categorical_column=fc_weekofyear, dimension=NEMBEDS)\n",
    "    fc_embed_crossed_day_hr = tf.feature_column.embedding_column(fc_crossed_day_hr, dimension=NEMBEDS)\n",
    "    fc_embed_crossed_pd_pair = tf.feature_column.embedding_column(fc_crossed_pd_pair, dimension=NEMBEDS)\n",
    "    fc_embed_crossed_hour_pu = tf.feature_column.embedding_column(fc_crossed_hour_pu, dimension=NEMBEDS)\n",
    "    fc_embed_crossed_hour_df = tf.feature_column.embedding_column(fc_crossed_hour_df, dimension=NEMBEDS)\n",
    "    \n",
    "    \n",
    "    deep = [\n",
    "        fc_distance,\n",
    "        fc_embed_weekofyear,\n",
    "        fc_embed_crossed_day_hr,\n",
    "        fc_embed_crossed_pd_pair,\n",
    "        fc_embed_crossed_hour_pu,\n",
    "        fc_embed_crossed_hour_df\n",
    "    ]\n",
    "    \n",
    "    return wide, deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving input reciver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek': tf.placeholder(dtype=tf.int64, shape=[None], name=\"dayofweek\"),\n",
    "        'hourofday': tf.placeholder(dtype=tf.int64, shape=[None], name=\"hourofday\"),\n",
    "        'weekofyear': tf.placeholder(dtype=tf.int64, shape=[None], name=\"weekofyear\"),\n",
    "        'pickup_zone_name': tf.placeholder(dtype=tf.string, shape=[None], name=\"pickup_zone_name\"),\n",
    "        'dropoff_zone_name': tf.placeholder(dtype=tf.string, shape=[None], name=\"dropoff_zone_name\"),\n",
    "        'centroide_distance': tf.placeholder(dtype=tf.float64, shape=[None], name=\"centroide_distance\"),\n",
    "        'flag_airport': tf.placeholder(dtype=tf.int64, shape=[None], name=\"flag_airport\"),\n",
    "        'flag_5_seat_car': tf.placeholder(dtype=tf.int64, shape=[None], name=\"flag_5_seat_car\"),\n",
    "        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['no_key']), [None], name=\"uuid\")\n",
    "    }\n",
    "    \n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1) for key, tensor in receiver_tensors.items()\n",
    "    }\n",
    "        \n",
    "    return tf.estimator.export.ServingInputReceiver(features=features, receiver_tensors=receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_estimator(output_dir, throttle_secs, nnsize, batch_size, train_steps, eval_steps, eval_delay_secs):\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_secs=throttle_secs,\n",
    "                                        tf_random_seed=2810,\n",
    "                                        keep_checkpoint_max=3)\n",
    "    \n",
    "    # Add custom evaluation metric\n",
    "    def my_rmse(labels, predictions):\n",
    "        pred_values = tf.squeeze(input=predictions[\"predictions\"], axis=-1)\n",
    "        return {\"rmse\": tf.compat.v1.metrics.root_mean_squared_error(labels=labels, predictions=pred_values)}\n",
    "    \n",
    "    # Feature engineering\n",
    "    wide, deep = get_wide_deep()\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir=output_dir,\n",
    "        linear_feature_columns=wide,\n",
    "        dnn_feature_columns=deep,\n",
    "        dnn_hidden_units=nnsize,\n",
    "        dnn_activation_fn=tf.nn.leaky_relu,\n",
    "        batch_norm=True,\n",
    "        dnn_dropout=0.2,\n",
    "        config=run_config)\n",
    "    \n",
    "    estimator = tf.estimator.add_metrics(estimator=estimator, metric_fn=my_rmse)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset('train', tf.estimator.ModeKeys.TRAIN, batch_size),\n",
    "        max_steps=train_steps)\n",
    "    \n",
    "    exporter = tf.estimator.BestExporter('exporter', serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset('test', tf.estimator.ModeKeys.EVAL, 2**15),\n",
    "        steps=eval_steps,\n",
    "        start_delay_secs=eval_delay_secs,  # start evaluating after N seconds\n",
    "        throttle_secs=throttle_secs,  # evaluate every N seconds\n",
    "        exporters=exporter)\n",
    "    \n",
    "    return estimator, train_spec, eval_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BUCKET = \"edml\"\n",
    "OUTDIR = \"gs://{}/ai-platform/models/edml_trainer_{}\".format(BUCKET, datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "THROTTLE_SECS = 300\n",
    "NNSIZE = [1024, 512, 256]\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_STEPS = 100000 #5M per fare 3 apochs con batch_size 128 (input size 217M)\n",
    "EVAL_STEPS = 1 # None evals full eval dataset\n",
    "EVAL_DECAY_SECS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator, train_spec, eval_spec = my_estimator(output_dir=OUTDIR, throttle_secs=THROTTLE_SECS, nnsize=NNSIZE, batch_size=BATCH_SIZE, train_steps=TRAIN_STEPS,\n",
    "                                                eval_steps=EVAL_STEPS, eval_delay_secs=EVAL_DECAY_SECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, evaluate, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=read_dataset(\"2017\", tf.estimator.ModeKeys.TRAIN, BATCH_SIZE), max_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=read_dataset(\"2018\", tf.estimator.ModeKeys.EVAL, 2**15), steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = estimator.predict(input_fn=read_dataset(\"2019\", tf.estimator.ModeKeys.PREDICT, 2**15), yield_single_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i <= 10:\n",
    "    print(next(predictions))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
